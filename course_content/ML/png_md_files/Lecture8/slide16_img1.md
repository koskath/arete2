SG minimizes average of smooth functions:

1. There is a bullet point followed by the text "Function \( f_i(w) \) is error for example ‘i’."
   - The notation \( f_i(w) \) has "i" in subscript and is highlighted with a yellow background.
   - The letter "w" in \( f_i(w) \) is in red color.

2. Another bullet point begins the sentence "Iterations perform gradient descent on one random example ‘i’:"

3. Following this, there is a sequence " \( w^{t+1} = w^t - \alpha^t \nabla f_i(w^t) \)"
   - The notation \( w^{t+1} \) has "t+1" in superscript.
   - \( w^t \) has "t" in superscript.
   - \( \alpha^t \) has "t" in superscript and is in red color.
   - \( \nabla f_i(w^t) \) has "i" in subscript and is in blue color. The notation \( w^t \) within it has "t" in superscript.

4. Another bullet point reads "Cheap iterations even when ‘n’ is large, but doesn’t always decrease ‘f’."

5. The final bullet point states "Solves problem if \( \alpha^t \) goes to 0 at an appropriate rate."

6. Below this is a highlighted yellow section with the text "α = learning rate" with "α" in red color.

In the upper right corner, there is a formula:
- \( f(w) = \frac{1}{n} \sum_{i=1}^{n} f_i(w) \)
  - \( f(w) \) is on the left side of the equation.
  - The fraction \(\frac{1}{n}\) is followed by the summation symbol \(\sum_{i=1}^{n}\).
  - \( f_i(w) \) follows the summation symbol.