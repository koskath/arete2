# Slide 16 of Lecture 10 contains information about the Recurrent Network Training.

• Challenge of training RNN is vanishing and exploding gradient problem.
• Due to long-term dependencies or time instant of
unrolled recurrent neuron RNN suffers.
• Gradient clipping, BatchNorm and ReLU can be used for vanishing
and exploding gradient problem.
• (Exploding and vanishing gradients + discards early time instances) leads
to development of a memory cell called the Long Short-Term
Memory/LSTM.
16
