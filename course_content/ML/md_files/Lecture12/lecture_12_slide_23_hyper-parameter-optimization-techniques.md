# Slide 23 of Lecture 12 contains information about the Hyper-parameter Optimization Techniques.

• Babysitting or ‘Trial and Error’ or Grad student descent (GSD) method is implemented by
100% manual tuning and widely used.
• Problems due to large number of hyper-parameters, complex models, time-
consuming model evaluations, and non-linear hyper-parameter interactions.
• Grid search (GS) most commonly-used methods
• An exhaustive search or a brute-force method
• Works by evaluating the Cartesian product of user-specified finite set of values
• Problem: Inefficiency for high-dimensionality hyper-parameter configuration space
• Since number of evaluations increases exponentially to number of hyper-
parameters growth
Yang, Li, and Abdallah Shami. "On hyperparameter optimization of machine learning algorithms: Theory and practice."Neurocomputing415 (2020): 295-316.
