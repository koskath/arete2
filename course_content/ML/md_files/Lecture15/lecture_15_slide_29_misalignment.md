# Slide 29 of Lecture 15 contains information about the Misalignment.

• Two Causes of Misalignment:
• Reward Hacking: Proxy rewards are easy to optimize and measure, but suffers
from capturing full spectrum of actual rewards (misspecified rewards).
• Optimizing misspecified rewards lead to reward hacking.
• Goal misgeneralization: Agent actively pursues objectives distinct from training
objectives, while retaining capabilities acquired during training.
• Two primary factors of misalignment:
• Limitations of Human Feedback: During LLMs training
• inconsistencies can arise from human data annotators.
• may introduce biases deliberately.
• Limitations of Reward Modeling:
• Accurately capturing human values.
https://arxiv.org/pdf/2310.19852.pdf​
