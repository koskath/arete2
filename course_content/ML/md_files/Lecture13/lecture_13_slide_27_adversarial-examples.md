# Slide 27 of Lecture 13 contains information about the Adversarial Examples.

• A model is performing a task (e.g. classification) with some level of success.
• An adversary is attacking that model.
• Objective: to perform the task (maintaining a similar accuracy) under attack.
• Forms of adversarial image attacks:
• Untargeted adversarial attacks: cannot control output label of adversarial
image.
• Targeted adversarial attacks: can control output label of image.
Goodfellow et al. Explaining and Harnessing Adversarial Examples
