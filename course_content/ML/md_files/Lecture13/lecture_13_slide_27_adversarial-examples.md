# Slide 27 of Lecture 13 contains information about the Adversarial Examples.

In the adversarial setting, a model that normally performs tasks such as classification now faces an attacker who tries to induce errors, so the objective becomes maintaining comparable accuracy even under attack. Image-based attacks come in untargeted forms, where the adversary merely pushes the model toward any wrong label, and targeted forms, where the adversary forces a specific incorrect label.
