# Slide 26 of Lecture 13 contains information about the Adversarial Examples.

• ML models such as DNN, clustering, Naive Bayes, Decision tree, Multilayer Perceptron,
SVM are not attacked resilient.
• Injection of adversarial/malicious data into training datasets that can caused decreased
performance/ model failure is known as poisoning.
• Malicious users add malicious data with similar features of original data and wrong
labels.
• Malicious users might know the training data distribution; also learning algorithm.
Goodfellow et al. Explaining and Harnessing Adversarial Examples
