# Slide 16 of Lecture 13 contains information about the Convolution Layer Training Process.

The convolutional layer training process alternates forward propagation, which computes activations, with backpropagation, which passes gradients—denoted δ₁₁, δ₁₂, δ₂₁, δ₂₂, and so on—through the network to update convolutional filters, while pooling layers simply forward values and do not learn separate parameters. 