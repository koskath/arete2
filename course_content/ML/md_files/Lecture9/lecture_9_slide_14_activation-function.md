# Slide 14 of Lecture 9 contains information about the Activation Function.

• Defines how weighted sum of input is transformed to output from a node(s).
• All hidden layers use same activation function.
• Output layer use a different activation function from hidden layers.
• Large DNN with nonlinear activations can theoretically approximate any continuous
function.
• Sigmoid (S-shaped) activation function was good.
• ReLU works better in ANNs.
• Softmax ensure all estimated probabilities between 0 and 1 and they add up to 1
(required for exclusive classes).
16
Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurélien Géron, 2019.
