# Slide 39 of Lecture 9 contains information about the Rectified Linear Units (ReLU).

• ReLU mitigate the vanishing and exploding gradient problem.
• ReLU offers improvement on the tanh and sigmoid activation functions.
• ReLU works:
• Set the activation to 0 for values x < 0
• Set a linear slope of 1 when values x > 0
42
Bisong E. (2019) More on Optimization Techniques. In: Building Machine Learning and Deep Learning Models on Google Cloud Platform. Apress, Berkeley, CA
